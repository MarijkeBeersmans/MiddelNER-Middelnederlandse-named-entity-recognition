{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb77328",
   "metadata": {},
   "source": [
    "# SpaCy en Middelnederlands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c387e",
   "metadata": {},
   "source": [
    "SpaCy is een Open Source natural language processing library. Het is een bekende en performante tool voor onder andere tokenization, part-of-speech tagging, named entity recognition en parsing en ondersteunt meerdere, moderne talen. In dit notebook wordt er kort onderzocht hoe goed SpaCy kan gebruikt worden voor Middelnederlands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "877dbefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA ORG\n",
      "Elon Musk’s PERSON\n",
      "$2.9 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "# !pip install spacy\n",
    "# !pip install \"https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\"\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"NASA awarded Elon Musk’s SpaceX a $2.9 billion contract to build the lunar lander.\")\n",
    "\n",
    "#\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,  ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a66600a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sente Baues PERSON\n",
      "Gilise den Scoutete PERSON\n",
      "Seggen GPE\n",
      "Seggen GPE\n",
      "Sente Baues PERSON\n",
      "Soens Shelechs PERSON\n",
      "Scoutete es Sculdech Scepenen LOC\n",
      "Ende Dabt GPE\n",
      "Newaer PERSON\n",
      "Scoutete FAC\n",
      "Newaer GPE\n",
      "Scepenen PERSON\n",
      "Also PERSON\n",
      "Newaer GPE\n",
      "Sekerhede PERSON\n",
      "Scoutete PERSON\n",
      "Worde GPE\n",
      "Dabt den Scoutete PERSON\n",
      "Ende de Scoutete PERSON\n",
      "Sabbets PERSON\n",
      "Ende GPE\n",
      "Ware PERSON\n",
      "Scepenen PERSON\n",
      "Ende GPE\n",
      "Scoutete PERSON\n",
      "Scoutete PERSON\n",
      "Newaer GPE\n",
      "Ende GPE\n",
      "Soudet de bode ORG\n",
      "Dabt PERSON\n",
      "Scoutete PERSON\n",
      "Van PERSON\n",
      "Scoutete FAC\n",
      "Name PERSON\n",
      "Scoutete FAC\n",
      "Bleue GPE\n",
      "Scepenen PERSON\n",
      "Scoutete FAC\n",
      "Negeen PERSON\n",
      "Ende GPE\n",
      "Gebiedene GPE\n",
      "Siene GPE\n",
      "Scepenen PERSON\n",
      "Scepenen PERSON\n",
      "derde ORDINAL\n",
      "Scoutete LOC\n",
      "Scoutete LOC\n",
      "Scepenen PERSON\n",
      "Scepenen PERSON\n",
      "Scepenen PERSON\n",
      "Alse NORP\n",
      "Scoutete PERSON\n",
      "Scoutete PERSON\n",
      "Sente Bauen GPE\n",
      "dienstliede CARDINAL\n",
      "Ende i d in Sente WORK_OF_ART\n",
      "Scoutete LOC\n",
      "Sitten bi den ontfangere met scepenen kiesen tgelt name hit ongaue hi WORK_OF_ART\n",
      "Gans NORP\n",
      "Sitten bi den ontfangere met scepenen kiesen tgelt doent goet waert ongaue WORK_OF_ART\n",
      "An PERSON\n",
      "Scoutete FAC\n",
      "Gasthus GPE\n",
      "An PERSON\n",
      "Sente Baues PERSON\n",
      "Scoutete LOC\n",
      "Scoutete FAC\n",
      "Ginghirejegen GPE\n",
      "Seghere Deken van Ghent PERSON\n",
      "Segel PERSON\n",
      "Smanendages PERSON\n",
      "Sente Baues PERSON\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download nl_core_news_sm\n",
    "import spacy\n",
    "\n",
    "# nlp = spacy.load(\"nl_core_news_sm\")\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "text = \"Ombe tujst die was tusschen den Gotshuse van Sente Baues Gilise den Scoutete uan Sente Baues te beuelne ombedat elc wilde weten sin recht So Ghingensis bede up minsser willems Seggen van maldengem minsSer huges Seggen uan steenlant up volcouts meiers seggen van Sente Baues si hebben elkerlijcs recht ondersceden also alse hier es gescreuen Jn de name suader Soens Shelechs gheests Ende si hebben gheseit Dat de Scoutete es Sculdech Scepenen te maenne jn allen steden daer mense sculdech es te manne Ende Dabt es Sculdech daer te hebbene sinen bode daer te wesene alse here Ende dingere jof hi wille Ende also ghelike uan den eeden te stauene Newaer de Scoutete en mach deede njet stauen also alse hi wille Newaer also alse Scepenen recht dinct Also gelike uan zekerheden tontfane Newaer alse sulke Sekerhede alse den abt genoeget moet den Scoutete genoegen van onghewisden dingen behouden sinen rechte Dat recht es Worde mesdaet gewiset an dien man dat Dabt den Scoutete antworde dies mans jof sins deels uan der mesdaet Ende de Scoutete en mach njemene borgen no laten gaen die hem wert gewiset te houdene bi wette van ongewisden dingen sonder Sabbets orlof het ne ware uan hauen Ende de scoutete nes sculdech te houdene negenen man dien dabt uaet bi heereden jof bi mannevonnesse Ware in eregherstede dabt jof sin bode scepenen daer de scoutete njet ne ware So mochte dabt jof sin bode maken enen maenre up die wile met Scepenen behouden scouteten rechte Ende waert uan onversinliken dingen so Soudemen soeken den Scoutete jof sinen bode thenen hus dat hi core int dorp dat hus moet hi nomen van jare te jare Ende waert ombe sake die geboden ware ter kerken up nameliken dach met scepenen daerombe en soudemen den Scoutete njet soeken Newaer maken enen maenre alse vorseit es Ende worde tujst jof de scoutete gesocht ware jof ne ware so Soudet de bode nemen up sinen eet jof hine sochte sonder meer toe te doene Dabt mach maken leedsman van sinen heere wienso hi wille de scoutete moet medeuaren alse Scoutete hebben ghelijc andren mannen Van manne te ontliuene jof lijctekin te makene dat es sculdech de Scoutete te doene jof sin bode Ende ware iemen gewiset te ontliuene jof lijctekin te makene dat mach dabt quite laten sonder den scoutete het neware jof dabt ghelt deraf name Name dabt ghelt deraf so ware de Scoutete sculdech te hebbene sin derdendeel uan der mesdaet Bleue enech man in hachten metten scoutete bi wisdome uan Scepenen van dien man es de Scoutete sculdech te hebbene xii d elx dages njet meer daerombe moet hi hem geuen redenlike behoefte Negeen gebot es de scoutete sculdech te gebiedene sonder den abt jof sinen bode sonder sin gheheet het ne ware loke te scouwene waterlaet daer mach dabt hebben sinen bode wil hi te siene jof men recht doet de scoutete moet gebieden scouwen updietijtdat het scepenen nutte dinct Ende lant te Gebiedene pande te uerbiedene ende ten derden gebode uan den panden eiset de scoutete sculdech te latene weten den abt jof sinen bode ombe te Siene jof de pande iet beter sin dane tgelt daer mense ouer heft Van allen mesdaden van iij sol die Scepenen wisen jof hoeso si uallen es de scoutete sculdech te hebbene den derden penninc njet meer sonder uan loken te scouwene waterlate van hachtingen van pandingen dat daeraf Scepenen wisen dat es Scouteten allene tote iij sol Ende van allen meerre saken es de derde penninc Scouteten Ende also gelike van allen goede dat men vint Ende worde leuende beeste uonden die soudemen houden met gemenen rade der heerscepe der Scepenen metten minsten coste totediendatter wet mede ware gedaen loke te scouwene waterlaet dat es Scouteten recht up doude njet vorder ende dabt mach daer hebben sinen bode wilhi ende geuielt dat men vorder scouwen soude loke jof waterlaet so waer het scouteten recht njemens el gelijc den ouden te scouwene Neghene hachtinge mach de scoutete doen sonder scepenen ende sabbets bode jof hire wesen wille het ne ware van uremden van vluchtegen jof die tujst hadde gemaect daer mach de Scoutete hant ane Slaen sonder scepenen sabbets bode die moet hi bringen voruoets vor scepenen sabbets bode alse wet mede te doene Ende sabbets bode moet de Scoutete soeken ter kemenaden en vondemen sabbets bode njet jof en wilde hi niet comen so mach de scoutete die hachtinge wel doen bi maningen uan scepenen sonder sabbets bode Ende worde tujst ombe tsoeken dat soude de bode nemen up sinen eet dat hine daer sochte sonder meer toe te doene Negene pandinge mach de scoutete doen sonder Scepenen sabbets bode jof hire wesen wille dien bode moet de scoutete eschen ter kemenaden ne vondemene njet jof en wilde hi njet comen so mach de scoutete panden met Scepenen sonder Sabbets bode Vp goet uan der costrijen jof uan der kelrijen jof der fermerien ne mach de scoutete njet panden het ne ware ombe gewisde mesdaet jof ombe dinc die men cuerde bi gemenen rade van Scepenen No hachtinge doen het ne ware uan vluchtegen goede ende dat uremder liede ware jof uan tujste Alse men gebiet gedeel daer es sculdech te wesene de scoutete scepenen sabbets bode up dien dach jof hire wesen wille Ende daeraf es men den Scoutete scepenen sculdech te voedene i mael redenleke of sijt nemen willen Jn allen desen steden die hier uornoemt sin so mach de Scoutete setten sinen bode bi wette te doene gelijc hem Seluen Alle de ghene die behoren te Sente Bauen te ij d jaerlix geboren sin binnen der vogedien wonen derinne dat es Scouteten recht uan dien ij d jaerlix vi d alsi huwen xii d alsi steruen Alle degene die staen ten besten hoefstoele dat es scouteten recht alsi steruen ij sol an den besten hoefstoel sonder van den genen die stonden ten besten hoefstoele eer dabt alle sine dienstliede Sette ten besten hoefstoele Van den maendachlande van den wedelande dat es scouteten recht i d jn wittendonresdage van den bunre Ende i d uan iij bunren in meidage Ende i d in Sente martinsdage van den bunre Van desen es sculdech de Scoutete viii sol jn wittendonresdage viij sol in Sente martinsdage den abt Van elker balchfart es scouteten recht ij sol daerombe moet hi Sitten bi den ontfangere met scepenen kiesen tgelt name hit ongaue hi eist sculdech goet te doene Van den groten chense es scouteten recht iiij sol ende i Gans metten geroue i virdale wins ij herenbrot daerombe moet hi Sitten bi den ontfangere met scepenen kiesen tgelt doent goet waert ongaue An de vetteme es scouteten recht iiij sol alse lange alse mense neemt lietese Dabt quite so ne haddere de Scoutete njet an Jn kermesdage es Scouteten recht tetene upt Gasthus met ij cnapen niet meer alse sulke spise alse men andren cnapen geuet An de scutterie van mendonc no van Sente Baues so ne heft de Scoutete geen recht Ende lant heft de Scoutete te lene van den Gotshuse alse uele alse hi bi wette mach betogen Wi vinderen die hier uornoemt sin seggen dat dit dat hier gescreuen es es scouteten leen wi seggen dat hit aldus alset hier gescreuen staet es Sculdech te houdene Ginghirejegen dat moeste hi boeten betren also alse den mannen recht dochte bi gemaenden eede Jn orconscepe van al desen So hebict willem here van maldengem ic huge die men heet here van steenlant riddren ic Seghere Deken van Ghent bi volcouts meiers bede van sente baues ombedat hi negenen Segel heft besegelt met onsen Segelen dit was besegelt besceden jnt jaer van den carnatione mcctuendeviftech Smanendages na reminiscere ombe de meerre vastheden heuet tcoeuent dabt van Sente Baues besegelt met haren Segelen\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,  ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c223841",
   "metadata": {},
   "source": [
    "## Various spacy tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eea6a6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os #directories\n",
    "import pprint\n",
    "import lxml\n",
    "import lxml.etree as etree\n",
    "\n",
    "path = 'C14NL-PoS/C14NL-PoS'\n",
    "\n",
    "def print_xml(element):\n",
    "    \"functie om xml mooi te printen\"\n",
    "    print(etree.tostring(element, pretty_print=True).decode('utf-8'))\n",
    "    \n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f2690b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wie Robrecht van den walle Jhan van den Walle Jhan hauwiel Jacob van den werue ende wille danscoe makere scepenen van den ypscen ambochte Doen te wetene allen lieden Dat pauwels van inghelant En clais withoc porters jn ype Ebben jcoght jeghen jhan de brueselare en cres-tine zijn wijf poerts jn ypre ouer timene van den scamel werken van yp Een leniene jmeitte lants En een alue lettel min of meer En alle de catelen diere up staen en der toe behoren ripe en groene Ert vast wertel vast en naghel vast licghende en staende an de zuut zide jnt hof dat was shen niclais van der caerde wan of dat hem de vcops kennen en sien wel verghouden Ende Ebbens halm en ghifte jworpen ten jmene boef van den scamelweken van yp vornomt wel en wettelike alse coustumen en vsagen zijn binnen den ypscen ambachte En se ebbens hemleden jwet jwersceip van allen calengen jeghen allen lieden qte lanc ome qte ghelt behouden shen chens van den lande diere vte gaet jn or-conscepe van dezer kennesse en van dezen alme wie scepen vornomt ebben an dezen chart jdaen onze zegle vuthanghende jmaect jnt jaer van Cirdcie . m.ccc. en achte En twintegh jn laumaent Saterdachs na sinte pauwels dach\n",
      "['Robrecht', 'van den walle', 'Jhan', 'van den Walle', 'Jhan', 'hauwiel', 'Jacob', 'van den werue', 'wille', 'danscoe makere', 'pauwels', 'van inghelant', 'clais', 'withoc', 'ype', 'jhan', 'de brueselare', 'cres-tine', 'ypre', 'yp', 'niclais', 'van der caerde', 'yp', 'Cirdcie', 'laumaent', 'sinte pauwels dach']\n",
      "Wie {Robrecht} {van den walle} {Jhan} {van den Walle} {Jhan} {hauwiel} {Jacob} {van den werue} ende {wille} {danscoe makere} scepenen van den ypscen ambochte Doen te wetene allen lieden Dat {pauwels} {van inghelant} En {clais} {withoc} porters jn {ype} Ebben jcoght jeghen {jhan} {de brueselare} en {cres-tine} zijn wijf poerts jn {ypre} ouer timene van den scamel werken van {yp} Een leniene jmeitte lants En een alue lettel min of meer En alle de catelen diere up staen en der toe behoren ripe en groene Ert vast wertel vast en naghel vast licghende en staende an de zuut zide jnt hof dat was shen {niclais} {van der caerde} wan of dat hem de vcops kennen en sien wel verghouden Ende Ebbens halm en ghifte jworpen ten jmene boef van den scamelweken van {yp} vornomt wel en wettelike alse coustumen en vsagen zijn binnen den ypscen ambachte En se ebbens hemleden jwet jwersceip van allen calengen jeghen allen lieden qte lanc ome qte ghelt behouden shen chens van den lande diere vte gaet jn or-conscepe van dezer kennesse en van dezen alme wie scepen vornomt ebben an dezen chart jdaen onze zegle vuthanghende jmaect jnt jaer van {Cirdcie} . m.ccc. en achte En twintegh jn {laumaent} Saterdachs na {sinte pauwels dach}\n",
      "N072p32901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DocId</th>\n",
       "      <th>text</th>\n",
       "      <th>ents</th>\n",
       "      <th>body with ents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N069p33301</td>\n",
       "      <td>WiE pauwels vlaminc mich de ghier fense ghoedy...</td>\n",
       "      <td>[pauwels, vlaminc, mich, de ghier, fense, ghoe...</td>\n",
       "      <td>WiE {pauwels} {vlaminc} {mich} {de ghier} {fen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N072p32101</td>\n",
       "      <td>Wie heinriq de hane lambt van der veste Reubre...</td>\n",
       "      <td>[heinriq, de hane, lambt, van der veste, Reubr...</td>\n",
       "      <td>Wie {heinriq} {de hane} {lambt} {van der veste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N072p32901</td>\n",
       "      <td>Wie Robrecht van den walle Jhan van den Walle ...</td>\n",
       "      <td>[Robrecht, van den walle, Jhan, van den Walle,...</td>\n",
       "      <td>Wie {Robrecht} {van den walle} {Jhan} {van den...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DocId                                               text  \\\n",
       "0  N069p33301  WiE pauwels vlaminc mich de ghier fense ghoedy...   \n",
       "1  N072p32101  Wie heinriq de hane lambt van der veste Reubre...   \n",
       "2  N072p32901  Wie Robrecht van den walle Jhan van den Walle ...   \n",
       "\n",
       "                                                ents  \\\n",
       "0  [pauwels, vlaminc, mich, de ghier, fense, ghoe...   \n",
       "1  [heinriq, de hane, lambt, van der veste, Reubr...   \n",
       "2  [Robrecht, van den walle, Jhan, van den Walle,...   \n",
       "\n",
       "                                      body with ents  \n",
       "0  WiE {pauwels} {vlaminc} {mich} {de ghier} {fen...  \n",
       "1  Wie {heinriq} {de hane} {lambt} {van der veste...  \n",
       "2  Wie {Robrecht} {van den walle} {Jhan} {van den...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = []\n",
    "\n",
    "def get_flattext(tree):\n",
    "    body = []\n",
    "    for w in tree.iterfind('.//w'):\n",
    "        body.append(''.join(w.itertext()).strip().replace('\\n               ', ''))\n",
    "    flattext = ' '.join(body)\n",
    "    return flattext\n",
    "\n",
    "def get_ents_list(tree):\n",
    "    ents = []\n",
    "    for w in tree.iterfind('.//w'):\n",
    "        try:\n",
    "            if w.attrib['ana'].find('c02') != -1:\n",
    "                txt_ent = ''.join(w.itertext()) \n",
    "                ents.append(txt_ent.strip().replace('\\n               ', ''))\n",
    "        except KeyError:\n",
    "            continue\n",
    "    return ents\n",
    "\n",
    "def get_text_with_ents(tree):\n",
    "    body_with_ents = []\n",
    "    for w in tree.iterfind('.//w'):\n",
    "        try:\n",
    "            if w.attrib['ana'].find('c02') != -1:\n",
    "                txt_ent = ''.join(w.itertext()).strip()\n",
    "                txt_ent = '{' + txt_ent + '}'\n",
    "                body_with_ents.append(txt_ent)\n",
    "            else:\n",
    "                body_with_ents.append(''.join(w.itertext()).strip().replace('\\n               ', ''))\n",
    "        except KeyError:\n",
    "            body_with_ents.append(''.join(w.itertext()).strip().replace('\\n               ', ''))\n",
    "    return ' '.join(body_with_ents)\n",
    "\n",
    "# def ents_with_index(tree):\n",
    "#     body = []\n",
    "#     ents_with_index = []\n",
    "#     for w in tree.iterfind('.//w'):\n",
    "#         try:\n",
    "#             if w.attrib['ana'].find('c02') != -1:\n",
    "#                 txt_ent = ''.join(w.itertext()).strip()\n",
    "#                 ent_with_index = (ent, len(body), (len(body) + len(txt_ent.split())))\n",
    "#                 body.append(txt_ent)\n",
    "#                 ents_with_index.append(ent_with_index)\n",
    "#             else:\n",
    "#                 body.append(''.join(w.itertext()).strip().replace('\\n               ', ''))\n",
    "#         except KeyError:\n",
    "#             body.append(''.join(w.itertext()).strip().replace('\\n               ', ''))\n",
    "#     return ents_with_index\n",
    "    \n",
    "    \n",
    "for filename in files[5:8]:\n",
    "    with open(path + \"/\" + filename, \"r\", encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "        tree = etree.parse(f)\n",
    "        flattext = get_flattext(tree)\n",
    "        ents = get_ents_list(tree)\n",
    "        body_with_ents = get_text_with_ents(tree)\n",
    "#         ents_with_index = ents_with_index(tree)\n",
    "        row = [filename[:-4], flattext, ents, body_with_ents]\n",
    "        test.append(row)\n",
    "\n",
    "testje = pd.DataFrame(test)\n",
    "testje.columns= ['DocId', 'text', 'ents', 'body with ents']\n",
    "\n",
    "\n",
    "print(testje['text'][2])\n",
    "print(testje['ents'][2])\n",
    "# print(testje['ents with index'][2])\n",
    "print(testje['body with ents'][2])\n",
    "print(testje['DocId'][2])\n",
    "testje.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "780252f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = []\n",
    "    \n",
    "# for filename in files[5:8]:\n",
    "#     with open(path + \"/\" + filename, \"r\", encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "#         row = []\n",
    "#         #we willen de \"lopende tekst\"\n",
    "#         tree = etree.parse(f)\n",
    "# #     print_xml(tree)\n",
    "# #         body = [w.text if len(w.getchildren()) == 0 else child.text for child in w.getchildren() for w in tree.iterfind('.//w')]\n",
    "#         body = []\n",
    "#         for w in tree.iterfind('.//w'):\n",
    "#             body.append(''.join(w.itertext()).strip())\n",
    "# # print_xml(body)\n",
    "#         flattext = ' '.join(body)\n",
    "# #         print(flattext)\n",
    "# #         flax = flattext.replace(\"\\n               \", '')\n",
    "#         #get named entities\n",
    "#         ents = []\n",
    "#         for w in tree.iterfind('.//w'):\n",
    "#             try:\n",
    "#                 if w.attrib['ana'].find('c02') != -1:\n",
    "#                     txt_ent = ''.join(w.itertext()) \n",
    "#                     ents.append(txt_ent.strip())\n",
    "#                     #replace('\\n', '')\n",
    "# #                     if len(w.getchildren) != 0:\n",
    "# #                         for child in w.getchildren\n",
    "#             except KeyError:\n",
    "#                 continue\n",
    "#         body_with_ents = []\n",
    "#         for w in tree.iterfind('.//w'):\n",
    "#             try:\n",
    "#                 if w.attrib['ana'].find('c02') != -1:\n",
    "#                     txt_ent = ''.join(w.itertext()).strip()\n",
    "#                     txt_ent = '{' + txt_ent + '}'\n",
    "#                     body_with_ents.append(txt_ent)\n",
    "#                 else:\n",
    "#                     body_with_ents.append(''.join(w.itertext()).strip())\n",
    "#             except KeyError:\n",
    "#                 body_with_ents.append(''.join(w.itertext()).strip())\n",
    "#         row.append(filename[:-4])\n",
    "#         row.append(flattext)\n",
    "#         row.append(ents)\n",
    "#         row.append(' '.join(body_with_ents))\n",
    "#         test.append(row)\n",
    "        \n",
    "# #     abbr = tree.iterfind('//abbr')\n",
    "# #     for ab in abbr:\n",
    "# #         print(ab.attrib['expan'])\n",
    "\n",
    "# testje = pd.DataFrame(test)\n",
    "# testje.columns= ['DocId', 'text', 'ents', 'body with ents']\n",
    "\n",
    "\n",
    "# print(testje['text'][2])\n",
    "# print(testje['ents'][2])\n",
    "# print(testje['body with ents'][2])\n",
    "# print(testje['DocId'][2])\n",
    "# testje.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c500d200",
   "metadata": {},
   "source": [
    "Wat heb ik nodig:\n",
    "- een lijst met spacy's PoS-tags\n",
    "- een dictionary die de code van de KANTL vervangt in leesbare PoS-tags en dan in spacy's PoS-tags\n",
    "- daarna kan ik met het volgende codeblok gaan kijken of in mijn dataset eventueel en de veranderingen maken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6949ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#         with open(path + \"/\" + entry.name, \"r\", encoding=\"UTF-8\", errors=\"ignore\") as f:\n",
    "#             tree = etree.parse(f)\n",
    "#             for w in tree.iterfind('.//w'):\n",
    "#                 #the if handles the abbreviations (they are retained as they appear in the text, not in expanded form)\n",
    "#                 if len(w.getchildren()) != 0:\n",
    "#                     children=w.getchildren()\n",
    "#                     for child in children:\n",
    "#                         try:\n",
    "#                             if w.attrib['ana'].startswith('c02') and euhm[-1][-1] == 'B':\n",
    "#                                 row = [entry.name[:-4], child.text, w.attrib['lemma'], w.attrib['ana'], 'I']\n",
    "#                                 euhm.append(row)\n",
    "#                             elif w.attrib['ana'].startswith('c02'):\n",
    "#                                 row = [entry.name[:-4], child.text, w.attrib['lemma'], w.attrib['ana'], 'B']\n",
    "#                                 euhm.append(row)\n",
    "#                             else:\n",
    "#                                 row = [entry.name[:-4], child.text, w.attrib['lemma'], w.attrib['ana'], 'O']\n",
    "#                                 euhm.append(row)\n",
    "#                         except KeyError:\n",
    "#                             euhm.append([entry.name[:-4], child.text, 'X', 'X', 'O'])\n",
    "#                 #the else statement handles the regular text\n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         if len(w.attrib['ana'].split()) == 1:\n",
    "#                             if w.attrib['ana'].startswith('c02') and euhm[-1][-1] == 'B':\n",
    "#                                 row = [entry.name[:-4], w.text, w.attrib['lemma'], w.attrib['ana'], 'I']\n",
    "#                                 euhm.append(row)\n",
    "#                             elif w.attrib['ana'].startswith('c02'):\n",
    "#                                 row = [entry.name[:-4], w.text, w.attrib['lemma'], w.attrib['ana'], 'B']\n",
    "#                                 euhm.append(row)\n",
    "#                             else:\n",
    "#                                 row = [entry.name[:-4], w.text, w.attrib['lemma'], w.attrib['ana'], 'O']\n",
    "#                                 euhm.append(row)\n",
    "#                         else:\n",
    "#                             rows = handle_weird_elements(w)\n",
    "#                             for row in rows:\n",
    "#                                 euhm.append([entry.name[:-4]] + list(row))\n",
    "#                     except KeyError:\n",
    "#                         euhm.append([entry.name[:-4], w.text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98e2a321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x000002E3D990C450>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x000002E3D9B5D900>), ('tagger', <spacy.pipeline.tagger.Tagger object at 0x000002E3DE919C20>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x000002E3DD837640>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x000002E3D97FCA00>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x000002E3D9E40840>), ('lemmatizer', <spacy.lang.nl.lemmatizer.DutchLemmatizer object at 0x000002E3DE72B840>)]\n",
      "['tok2vec', 'morphologizer', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "# Load the tagger and parser but don't enable them\n",
    "# nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "# # Explicitly enable the tagger later on\n",
    "# nlp.enable_pipe(\"tagger\")\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "# nlp = spacy.load(\"nl_core_news_lg\")\n",
    "\n",
    "print(nlp.pipeline)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68798fe7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3079\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3080\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-21c93d4b20bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nl_core_news_sm\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestje\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# for token in doc:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3022\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3023\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3024\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3025\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3080\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "# for token in doc:\n",
    "#     print(token, '-', token.pos_, '-', token.lemma_, '-', token.ent_type_, '-', token.ent_iob_)\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "doc = nlp(testje['text'][0])\n",
    "\n",
    "# for token in doc:\n",
    "#         if token.ent_iob_ != 'O':\n",
    "#             print(token, '-', token.ent_type_, '-', token.ent_iob_)\n",
    "            \n",
    "# testje['ents-vanilla-SpaCy'] = [nlp(text).ents for text in testje['text']]\n",
    "\n",
    "spacy.displacy.render(doc, style=\"ent\")\n",
    "testje['body with ents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b7e143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# (testje['ents'], '-', testje['ents-vanilla-SpaCy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0488a9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08bd4e27",
   "metadata": {},
   "source": [
    "## Test met SpaCy's Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea8d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list with first and last names\n",
    "firstnames = pd.read_csv('christian_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "lastnames = pd.read_csv('family_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "firstnames = pd.read_csv('christian_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "lastnames = pd.read_csv('family_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "firstnames = list(firstnames[\"christian_name_document\"])\n",
    "firstname_patterns = [[{\"LOWER\" : firstname.casefold()}] for firstname in firstnames]\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"FIRSTNAME\", firstname_patterns)\n",
    "\n",
    "@Language.component(\"personal_name_component\")\n",
    "def firstname_comp_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    spans = [Span(doc, start, end, label=\"PERSON_FIRSTNAME\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(\"personal_name_component\", after=\"ner\")\n",
    "\n",
    "doc = nlp(txt)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040163cd",
   "metadata": {},
   "source": [
    "Het werkt redelijk goed met enkel de voornamen, maar eigenlijk moet ik meer testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6565406f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'LOWER': 'curcure'}], [{'LOWER': 'adalo'}], [{'LOWER': 'flagellus'}], [{'LOWER': 'blifhieblifhier'}], [{'LOWER': 'reinco'}], [{'LOWER': 'otburgis'}], [{'LOWER': 'le gravle grave'}], [{'LOWER': 'rotgardis'}], [{'LOWER': 'loikiloikin'}], [{'LOWER': 'boeventboevente'}], [{'LOWER': 'medardus'}], [{'LOWER': 'iohannis'}], [{'LOWER': 'rutcherus'}], [{'LOWER': 'le nieulierle nieuliers'}], [{'LOWER': 'sackelesackelet'}], [{'LOWER': 'juliot'}], [{'LOWER': 'smerl le poissoniers'}], [{'LOWER': 'rikeward'}], [{'LOWER': 'liuta'}], [{'LOWER': 'bounard'}]]\n",
      "['tok2vec', 'morphologizer', 'tagger', 'parser', 'ner', 'personal_name_component', 'attribute_ruler', 'lemmatizer']\n",
      "[('Robrecht', 'NAME'), ('Jacob', 'NAME'), ('clais', 'NAME')]\n",
      "['Robrecht', 'van den walle', 'Jhan', 'van den Walle', 'Jhan', 'hauwiel', 'Jacob', 'van den werue', 'wille', 'danscoe makere', 'pauwels', 'van inghelant', 'clais', 'withoc', 'ype', 'jhan', 'de brueselare', 'cres-tine', 'ypre', 'yp', 'niclais', 'van der caerde', 'yp', 'Cirdcie', 'laumaent', 'sinte pauwels dach']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "txt = testje['text'][2]\n",
    "\n",
    "firstnames = pd.read_csv('christian_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "lastnames = pd.read_csv('family_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "lastnames = list(lastnames[\"family_name_document\"])\n",
    "lastnames_imp = []\n",
    "for lastname in lastnames:\n",
    "    index = lastname.find(',')\n",
    "    if index:\n",
    "        assert type(index) == int\n",
    "        lastname = lastname[:index] + lastname[index+1:]\n",
    "        lastnames_imp.append(lastname)\n",
    "    else:\n",
    "        lastnames_imp.append(lastname)\n",
    "\n",
    "#load Dutch\n",
    "nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "names = list(firstnames[\"christian_name_document\"]) + lastnames_imp\n",
    "names = set(names)\n",
    "name_patterns = [[{\"LOWER\" : name.casefold()}] for name in names]\n",
    "\n",
    "print(name_patterns[:20])\n",
    "assert type(name_patterns) == list\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"NAME\", name_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"personal_name_component\")\n",
    "def firstname_comp_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"PERSON_NAME\"\n",
    "    spans = [Span(doc, start, end, label=\"NAME\") for match_id, start, end in matches]\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"personal_name_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(txt)\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(testje['ents'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "728b974b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'tagger', 'parser', 'ner', 'personal_name_component', 'last_name_component', 'attribute_ruler', 'lemmatizer']\n",
      "[pieter, gillis]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E1010] Unable to set entity information for token 17 which is included in more than one span in entities, blocked, missing or outside.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-16e96aff87ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m# Process the text and print the text and label for the doc.ents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    999\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE109\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1001\u001b[1;33m                 \u001b[0merror_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1002\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1003\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mraise_error\u001b[1;34m(proc_name, proc, docs, e)\u001b[0m\n\u001b[0;32m   1484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mraise_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproc_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1486\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m    994\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 996\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    997\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m                 \u001b[1;31m# This typically happens if a component is not initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-16e96aff87ca>\u001b[0m in \u001b[0;36mlastname_comp_function\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mwhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mspan\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mspans2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[0mwhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ments\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwhat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.ents.__set__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.set_ents\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E1010] Unable to set entity information for token 17 which is included in more than one span in entities, blocked, missing or outside."
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "# import re\n",
    "# from spacy.language import Language\n",
    "# from spacy.matcher import Matcher\n",
    "# from spacy.tokens import Span\n",
    "\n",
    "# firstnames = pd.read_csv('christian_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "# lastnames = pd.read_csv('family_name.txt', sep='\\t', header=0, encoding='UTF-16')\n",
    "\n",
    "# #load Dutch\n",
    "# nlp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "# firstnames = list(firstnames[\"christian_name_document\"])\n",
    "# firstname_patterns = [[{\"LOWER\" : firstname.casefold()}] for firstname in firstnames]\n",
    "\n",
    "# # print(firstname_patterns[:20])\n",
    "# # assert type(firstname_patterns) == list\n",
    "# # matcher = Matcher(nlp.vocab)\n",
    "# # matcher.add(\"FIRSTNAME\", firstname_patterns)\n",
    "\n",
    "# lastnames = list(lastnames[\"family_name_document\"])\n",
    "# lastnames_imp = []\n",
    "# for lastname in lastnames:\n",
    "#     index = lastname.find(',')\n",
    "#     if index:\n",
    "#         assert type(index) == int\n",
    "#         lastname = lastname[:index]\n",
    "#         lastnames_imp.append(lastname)\n",
    "#     else:\n",
    "#         lastnames_imp.append(lastname)\n",
    "        \n",
    "# lastname_patterns = [[{\"LOWER\" : lastname.casefold()}] for lastname in lastnames_imp]\n",
    "\n",
    "# assert type(firstname_patterns) == list\n",
    "# # matcher = Matcher(nlp.vocab)\n",
    "# # matcher.add(\"FIRSTNAME\", firstname_patterns)\n",
    "\n",
    "# # Define the custom component\n",
    "# @Language.component(\"personal_name_component\")\n",
    "# def firstname_comp_function(doc):\n",
    "#     # Apply the matcher to the doc\n",
    "#     matches = matcher(doc)\n",
    "# #     matches2 = matcher2(doc)\n",
    "#     # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "#     spans1 = [Span(doc, start, end, label=\"PERSON_FIRSTNAME\") for match_id, start, end in matches]\n",
    "# #     print(spans1[:20])\n",
    "# #     spans2 = [Span(doc, start, end, label=\"PERSON_LASTNAME\") for match_id, start, end in matches2]\n",
    "# #     print(spans2[:20])\n",
    "#     spans = spans1 #+ spans2\n",
    "# #     print(spans[:20])\n",
    "#     # Overwrite the doc.ents with the matched spans\n",
    "#     doc.ents = spans\n",
    "#     return doc\n",
    "\n",
    "# @Language.component(\"last_name_component\")\n",
    "# def lastname_comp_function(doc):\n",
    "#     # Apply the matcher to the doc\n",
    "# #     matches = matcher(doc)\n",
    "#     matches2 = matcher2(doc)\n",
    "#     # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "# #     spans1 = [Span(doc, start, end, label=\"PERSON_FIRSTNAME\") for match_id, start, end in matches]\n",
    "# # #     print(spans1[:20])\n",
    "#     spans2 = [Span(doc, start, end, label=\"PERSON_LASTNAME\") for match_id, start, end in matches2]\n",
    "#     print(spans2[:20])\n",
    "#     spans = spans2\n",
    "# #     print(spans[:20])\n",
    "#     # Overwrite the doc.ents with the matched spans\n",
    "#     what = (span for span in spans2 if span not in doc.ents)\n",
    "#     what = tuple(what)\n",
    "#     doc.ents = doc.ents + what\n",
    "#     return doc\n",
    "\n",
    "# # Add the component to the pipeline after the \"ner\" component\n",
    "# nlp.add_pipe(\"personal_name_component\", after=\"ner\")\n",
    "# nlp.add_pipe(\"last_name_component\", before=\"attribute_ruler\")\n",
    "# print(nlp.pipe_names)\n",
    "\n",
    "# # Process the text and print the text and label for the doc.ents\n",
    "# doc = nlp(txt)\n",
    "# print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d903e4",
   "metadata": {},
   "source": [
    "Dit werkt redelijk goed met enkel de firstnames. De achternamen zijn lastiger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956c5edb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
