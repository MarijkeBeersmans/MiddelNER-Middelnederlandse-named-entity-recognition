{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('spacy_errors.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE       0.86      0.78      0.82        40\n",
      "       B-LOC       0.81      0.65      0.72        83\n",
      "      B-PERS       0.90      0.86      0.88       256\n",
      "     B-MONEY       0.94      0.91      0.93        56\n",
      "      I-DATE       0.71      0.77      0.74       151\n",
      "       I-LOC       1.00      0.10      0.18        10\n",
      "      I-PERS       0.85      0.83      0.84       224\n",
      "     I-MONEY       0.87      0.97      0.92       128\n",
      "\n",
      "   micro avg       0.84      0.83      0.83       948\n",
      "   macro avg       0.87      0.73      0.75       948\n",
      "weighted avg       0.85      0.83      0.83       948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(list(df.true), list(df.pred), labels=['B-DATE', 'B-LOC', 'B-PERS', 'B-MONEY','I-DATE', 'I-LOC', 'I-PERS','I-MONEY']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7837514934289128\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DATE       0.64      0.57      0.61        40\n",
      "         LOC       0.81      0.65      0.72        83\n",
      "       MONEY       0.89      0.86      0.87        56\n",
      "        PERS       0.83      0.79      0.81       256\n",
      "\n",
      "   micro avg       0.82      0.75      0.78       435\n",
      "   macro avg       0.79      0.72      0.75       435\n",
      "weighted avg       0.81      0.75      0.78       435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "print(f1_score([list(df.true)], [list(df.pred)]))\n",
    "\n",
    "print(classification_report([list(df.true)], [list(df.pred)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = ''\n",
    "\n",
    "for x, y in zip(df.word, df.true):\n",
    "    true = true + (x.strip() + '\\t' + y.strip() + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ''\n",
    "\n",
    "for x, y in zip(df.word, df.pred):\n",
    "    pred = pred + (x.strip() + '\\t' + y.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5250 entries, 0 to 5249\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   word    5250 non-null   object\n",
      " 1   true    5250 non-null   object\n",
      " 2   pred    5250 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 164.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': {'correct': 360, 'incorrect': 14, 'partial': 0, 'missed': 61, 'spurious': 28, 'possible': 435, 'actual': 402, 'precision': 0.8955223880597015, 'recall': 0.8275862068965517, 'f1': 0.8602150537634409}, 'partial': {'correct': 331, 'incorrect': 0, 'partial': 43, 'missed': 61, 'spurious': 28, 'possible': 435, 'actual': 402, 'precision': 0.8768656716417911, 'recall': 0.8103448275862069, 'f1': 0.8422939068100358}, 'strict': {'correct': 328, 'incorrect': 46, 'partial': 0, 'missed': 61, 'spurious': 28, 'possible': 435, 'actual': 402, 'precision': 0.8159203980099502, 'recall': 0.7540229885057471, 'f1': 0.7837514934289128}, 'exact': {'correct': 331, 'incorrect': 43, 'partial': 0, 'missed': 61, 'spurious': 28, 'possible': 435, 'actual': 402, 'precision': 0.8233830845771144, 'recall': 0.7609195402298851, 'f1': 0.7909199522102747}}\n"
     ]
    }
   ],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=['LOC', 'PERS', 'DATE', 'MONEY'],  loader=\"conll\")\n",
    "\n",
    "# Returns overall metrics and metrics for each tag\n",
    "\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint as pp\n",
    "\n",
    "df2 = pd.DataFrame(results_per_tag['DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ent_type</th>\n",
       "      <th>partial</th>\n",
       "      <th>strict</th>\n",
       "      <th>exact</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incorrect</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>partial</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>missed</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spurious</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>possible</th>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>36.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>0.638889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.575000</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.605263</td>\n",
       "      <td>0.605263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ent_type    partial     strict      exact\n",
       "correct    32.000000  23.000000  23.000000  23.000000\n",
       "incorrect   1.000000   0.000000  10.000000  10.000000\n",
       "partial     0.000000  10.000000   0.000000   0.000000\n",
       "missed      7.000000   7.000000   7.000000   7.000000\n",
       "spurious    3.000000   3.000000   3.000000   3.000000\n",
       "possible   40.000000  40.000000  40.000000  40.000000\n",
       "actual     36.000000  36.000000  36.000000  36.000000\n",
       "precision   0.888889   0.777778   0.638889   0.638889\n",
       "recall      0.800000   0.700000   0.575000   0.575000\n",
       "f1          0.842105   0.736842   0.605263   0.605263"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Gebruiker/Documents/UA/Stage/NER-Evaluation/')\n",
    "\n",
    "from ner_evaluation.ner_eval import collect_named_entities, Entity\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper, compute_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "\n",
    "class Evaluator():\n",
    "\n",
    "    def __init__(self, true, pred, tags):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        if len(true) != len(pred):\n",
    "            raise ValueError(\"Number of predicted documents does not equal true\")\n",
    "\n",
    "        self.true = true\n",
    "        self.pred = pred\n",
    "        self.tags = tags\n",
    "\n",
    "        # Setup dict into which metrics will be stored.\n",
    "\n",
    "        self.metrics_results = {\n",
    "            'correct': 0,\n",
    "            'incorrect': 0,\n",
    "            'partial': 0,\n",
    "            'missed': 0,\n",
    "            'spurious': 0,\n",
    "            'possible': 0,\n",
    "            'actual': 0,\n",
    "            'precision': 0,\n",
    "            'recall': 0,\n",
    "        }\n",
    "\n",
    "        # Copy results dict to cover the four schemes.\n",
    "\n",
    "        self.results = {\n",
    "            'strict': deepcopy(self.metrics_results),\n",
    "            'ent_type': deepcopy(self.metrics_results),\n",
    "            'partial':deepcopy(self.metrics_results),\n",
    "            'exact':deepcopy(self.metrics_results),\n",
    "            }\n",
    "\n",
    "        # Create an accumulator to store results\n",
    "\n",
    "        self.evaluation_agg_entities_type = {e: deepcopy(self.results) for e in tags}\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        logging.info(\n",
    "            \"Imported %s predictions for %s true examples\",\n",
    "            len(self.pred), len(self.true)\n",
    "        )\n",
    "\n",
    "        for true_ents, pred_ents in zip(self.true, self.pred):\n",
    "\n",
    "            # Check that the length of the true and predicted examples are the\n",
    "            # same. This must be checked here, because another error may not\n",
    "            # be thrown if the lengths do not match.\n",
    "\n",
    "            if len(true_ents) != len(pred_ents):\n",
    "                raise ValueError(\"Prediction length does not match true example length\")\n",
    "\n",
    "            # Compute results for one message\n",
    "\n",
    "            tmp_results, tmp_agg_results = compute_metrics(\n",
    "                collect_named_entities(true_ents),\n",
    "                collect_named_entities(pred_ents),\n",
    "                self.tags\n",
    "            )\n",
    "\n",
    "            # Cycle through each result and accumulate\n",
    "\n",
    "            # TODO: Combine these loops below:\n",
    "\n",
    "            for eval_schema in self.results:\n",
    "\n",
    "                for metric in self.results[eval_schema]:\n",
    "\n",
    "                    self.results[eval_schema][metric] += tmp_results[eval_schema][metric]\n",
    "\n",
    "            # Calculate global precision and recall\n",
    "\n",
    "            self.results = compute_precision_recall_wrapper(self.results)\n",
    "\n",
    "            # Aggregate results by entity type\n",
    "\n",
    "            for e_type in self.tags:\n",
    "\n",
    "                for eval_schema in tmp_agg_results[e_type]:\n",
    "\n",
    "                    for metric in tmp_agg_results[e_type][eval_schema]:\n",
    "\n",
    "                        self.evaluation_agg_entities_type[e_type][eval_schema][metric] += tmp_agg_results[e_type][eval_schema][metric]\n",
    "\n",
    "                # Calculate precision recall at the individual entity level\n",
    "\n",
    "                self.evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(self.evaluation_agg_entities_type[e_type])\n",
    "\n",
    "        return self.results, self.evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def collect_named_entities(tokens):\n",
    "    \"\"\"\n",
    "    Creates a list of Entity named-tuples, storing the entity type and the start and end\n",
    "    offsets of the entity.\n",
    "    :param tokens: a list of tags\n",
    "    :return: a list of Entity named-tuples\n",
    "    \"\"\"\n",
    "\n",
    "    named_entities = []\n",
    "    start_offset = None\n",
    "    end_offset = None\n",
    "    ent_type = None\n",
    "\n",
    "    for offset, token_tag in enumerate(tokens):\n",
    "\n",
    "        if token_tag == 'O':\n",
    "            if ent_type is not None and start_offset is not None:\n",
    "                end_offset = offset - 1\n",
    "                named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "                start_offset = None\n",
    "                end_offset = None\n",
    "                ent_type = None\n",
    "\n",
    "        elif ent_type is None:\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "\n",
    "        elif ent_type != token_tag[2:] or (ent_type == token_tag[2:] and token_tag[:1] == 'B'):\n",
    "\n",
    "            end_offset = offset - 1\n",
    "            named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "\n",
    "            # start of a new entity\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "            end_offset = None\n",
    "\n",
    "    # catches an entity that goes up until the last token\n",
    "\n",
    "    if ent_type is not None and start_offset is not None and end_offset is None:\n",
    "        named_entities.append(Entity(ent_type, start_offset, len(tokens)-1))\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "def compute_metrics(true_named_entities, pred_named_entities, tags):\n",
    "\n",
    "\n",
    "    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    \n",
    "    evaluation = {\n",
    "        'strict': deepcopy(eval_metrics),\n",
    "        'ent_type': deepcopy(eval_metrics),\n",
    "        'partial': deepcopy(eval_metrics),\n",
    "        'exact': deepcopy(eval_metrics)\n",
    "    }\n",
    "\n",
    "    # results by entity type\n",
    "\n",
    "    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n",
    "\n",
    "    # keep track of entities that overlapped\n",
    "\n",
    "    true_which_overlapped_with_pred = []\n",
    "\n",
    "    # Subset into only the tags that we are interested in.\n",
    "    # NOTE: we remove the tags we don't want from both the predicted and the\n",
    "    # true entities. This covers the two cases where mismatches can occur:\n",
    "    #\n",
    "    # 1) Where the model predicts a tag that is not present in the true data\n",
    "    # 2) Where there is a tag in the true data that the model is not capable of\n",
    "    # predicting.\n",
    "\n",
    "    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n",
    "    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # Check each of the potential scenarios in turn. See\n",
    "        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "        # for scenario explanation.\n",
    "\n",
    "        # Scenario I: Exact match between true and pred\n",
    "\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['strict']['correct'] += 1\n",
    "            evaluation['ent_type']['correct'] += 1\n",
    "            evaluation['exact']['correct'] += 1\n",
    "            evaluation['partial']['correct'] += 1\n",
    "\n",
    "            # for the agg. by e_type results\n",
    "            evaluation_agg_entities_type[pred.e_type]['strict']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['ent_type']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['exact']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['partial']['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                pred_range = range(pred.start_offset, pred.end_offset)\n",
    "                true_range = range(true.start_offset, true.end_offset)\n",
    "\n",
    "                # Scenario IV: Offsets match, but entity type is wrong\n",
    "\n",
    "                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n",
    "                        and true.e_type != pred.e_type:\n",
    "\n",
    "                    # overall results\n",
    "                    evaluation['strict']['incorrect'] += 1\n",
    "                    evaluation['ent_type']['incorrect'] += 1\n",
    "                    evaluation['partial']['correct'] += 1\n",
    "                    evaluation['exact']['correct'] += 1\n",
    "\n",
    "                    # aggregated by entity type results\n",
    "                    evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['partial']['correct'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['exact']['correct'] += 1\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "                    found_overlap = True\n",
    "\n",
    "                    break\n",
    "\n",
    "                # check for an overlap i.e. not exact boundary match, with true entities\n",
    "\n",
    "                elif find_overlap(true_range, pred_range):\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    # Scenario V: There is an overlap (but offsets do not match\n",
    "                    # exactly), and the entity type is the same.\n",
    "                    # 2.1 overlaps with the same entity type\n",
    "\n",
    "                    if pred.e_type == true.e_type:\n",
    "\n",
    "                        # overall results\n",
    "                        evaluation['strict']['incorrect'] += 1\n",
    "                        evaluation['ent_type']['correct'] += 1\n",
    "                        evaluation['partial']['partial'] += 1\n",
    "                        evaluation['exact']['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['ent_type']['correct'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    # Scenario VI: Entities overlap, but the entity type is\n",
    "                    # different.\n",
    "\n",
    "                    else:\n",
    "                        # overall results\n",
    "                        evaluation['strict']['incorrect'] += 1\n",
    "                        evaluation['ent_type']['incorrect'] += 1\n",
    "                        evaluation['partial']['partial'] += 1\n",
    "                        evaluation['exact']['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        # Results against the true entity\n",
    "\n",
    "                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n",
    "\n",
    "                        # Results against the predicted entity\n",
    "\n",
    "                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "            # Scenario II: Entities are spurious (i.e., over-generated).\n",
    "\n",
    "            if not found_overlap:\n",
    "\n",
    "                # Overall results\n",
    "\n",
    "                evaluation['strict']['spurious'] += 1\n",
    "                evaluation['ent_type']['spurious'] += 1\n",
    "                evaluation['partial']['spurious'] += 1\n",
    "                evaluation['exact']['spurious'] += 1\n",
    "\n",
    "                # Aggregated by entity type results\n",
    "\n",
    "                # NOTE: when pred.e_type is not found in tags\n",
    "                # or when it simply does not appear in the test set, then it is\n",
    "                # spurious, but it is not clear where to assign it at the tag\n",
    "                # level. In this case, it is applied to all target_tags\n",
    "                # found in this example. This will mean that the sum of the\n",
    "                # evaluation_agg_entities will not equal evaluation.\n",
    "\n",
    "                for true in tags:                    \n",
    "\n",
    "                    evaluation_agg_entities_type[true]['strict']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['ent_type']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['partial']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['exact']['spurious'] += 1\n",
    "\n",
    "    # Scenario III: Entity was missed entirely.\n",
    "\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['strict']['missed'] += 1\n",
    "            evaluation['ent_type']['missed'] += 1\n",
    "            evaluation['partial']['missed'] += 1\n",
    "            evaluation['exact']['missed'] += 1\n",
    "\n",
    "            # for the agg. by e_type\n",
    "            evaluation_agg_entities_type[true.e_type]['strict']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['ent_type']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['partial']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['exact']['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n",
    "    # overall results, and use these to calculate precision and recall.\n",
    "\n",
    "    for eval_type in evaluation:\n",
    "        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n",
    "\n",
    "    # Compute 'possible', 'actual', and precision and recall on entity level\n",
    "    # results. Start by cycling through the accumulated results.\n",
    "\n",
    "    for entity_type, entity_level in evaluation_agg_entities_type.items():\n",
    "\n",
    "        # Cycle through the evaluation types for each dict containing entity\n",
    "        # level results.\n",
    "\n",
    "        for eval_type in entity_level:\n",
    "\n",
    "            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n",
    "                entity_level[eval_type]\n",
    "            )\n",
    "\n",
    "    return evaluation, evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def find_overlap(true_range, pred_range):\n",
    "    \"\"\"Find the overlap between two ranges\n",
    "    Find the overlap between two ranges. Return the overlapping values if\n",
    "    present, else return an empty set().\n",
    "    Examples:\n",
    "    >>> find_overlap((1, 2), (2, 3))\n",
    "    2\n",
    "    >>> find_overlap((1, 2), (3, 4))\n",
    "    set()\n",
    "    \"\"\"\n",
    "\n",
    "    true_set = set(true_range)\n",
    "    pred_set = set(pred_range)\n",
    "\n",
    "    overlaps = true_set.intersection(pred_set)\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def compute_actual_possible(results):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with actual, possible populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = results['correct']\n",
    "    incorrect = results['incorrect']\n",
    "    partial = results['partial']\n",
    "    missed = results['missed']\n",
    "    spurious = results['spurious']\n",
    "\n",
    "    # Possible: number annotations in the gold-standard which contribute to the\n",
    "    # final score\n",
    "\n",
    "    possible = correct + incorrect + partial + missed\n",
    "\n",
    "    # Actual: number of annotations produced by the NER system\n",
    "\n",
    "    actual = correct + incorrect + partial + spurious\n",
    "\n",
    "    results[\"actual\"] = actual\n",
    "    results[\"possible\"] = possible\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_recall(results, partial_or_type=False):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with precison and recall populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    actual = results[\"actual\"]\n",
    "    possible = results[\"possible\"]\n",
    "    partial = results['partial']\n",
    "    correct = results['correct']\n",
    "\n",
    "    if partial_or_type:\n",
    "        precision = (correct + 0.5 * partial) / actual if actual > 0 else 0\n",
    "        recall = (correct + 0.5 * partial) / possible if possible > 0 else 0\n",
    "\n",
    "    else:\n",
    "        precision = correct / actual if actual > 0 else 0\n",
    "        recall = correct / possible if possible > 0 else 0\n",
    "\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"recall\"] = recall\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_recall_wrapper(results):\n",
    "    \"\"\"\n",
    "    Wraps the compute_precision_recall function and runs on a dict of results\n",
    "    \"\"\"\n",
    "\n",
    "    results_a = {key: compute_precision_recall(value, True) for key, value in results.items() if\n",
    "                 key in ['partial', 'ent_type']}\n",
    "    results_b = {key: compute_precision_recall(value) for key, value in results.items() if\n",
    "                 key in ['strict', 'exact']}\n",
    "\n",
    "    results = {**results_a, **results_b}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "from collections import namedtuple\n",
    "from copy import deepcopy\n",
    "\n",
    "class Evaluator():\n",
    "\n",
    "    def __init__(self, true, pred, tags):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "        if len(true) != len(pred):\n",
    "            raise ValueError(\"Number of predicted documents does not equal true\")\n",
    "\n",
    "        self.true = true\n",
    "        self.pred = pred\n",
    "        self.tags = tags\n",
    "\n",
    "        # Setup dict into which metrics will be stored.\n",
    "\n",
    "        self.metrics_results = {\n",
    "            'correct': 0,\n",
    "            'incorrect': 0,\n",
    "            'partial': 0,\n",
    "            'missed': 0,\n",
    "            'spurious': 0,\n",
    "            'possible': 0,\n",
    "            'actual': 0,\n",
    "            'precision': 0,\n",
    "            'recall': 0,\n",
    "        }\n",
    "\n",
    "        # Copy results dict to cover the four schemes.\n",
    "\n",
    "        self.results = deepcopy(self.metrics_results)\n",
    "\n",
    "        # Create an accumulator to store results\n",
    "\n",
    "        self.evaluation_agg_entities_type = {e: deepcopy(self.results) for e in tags}\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "\n",
    "        logging.info(\n",
    "            \"Imported %s predictions for %s true examples\",\n",
    "            len(self.pred), len(self.true)\n",
    "        )\n",
    "\n",
    "        for true_ents, pred_ents in zip(self.true, self.pred):\n",
    "\n",
    "            # Check that the length of the true and predicted examples are the\n",
    "            # same. This must be checked here, because another error may not\n",
    "            # be thrown if the lengths do not match.\n",
    "\n",
    "            if len(true_ents) != len(pred_ents):\n",
    "                raise ValueError(\"Prediction length does not match true example length\")\n",
    "\n",
    "            # Compute results for one message\n",
    "\n",
    "            tmp_results, tmp_agg_results = compute_metrics(\n",
    "                collect_named_entities(true_ents),\n",
    "                collect_named_entities(pred_ents),\n",
    "                self.tags\n",
    "            )\n",
    "\n",
    "            # Cycle through each result and accumulate\n",
    "\n",
    "            # TODO: Combine these loops below:\n",
    "\n",
    "\n",
    "            for metric in self.results:\n",
    "\n",
    "                self.results[metric] += tmp_results[metric]\n",
    "\n",
    "            # # Calculate global precision and recall\n",
    "\n",
    "            # self.results = compute_precision_recall_wrapper(self.results)\n",
    "\n",
    "            # Aggregate results by entity type\n",
    "\n",
    "            for e_type in self.tags:\n",
    "\n",
    "                for metric in tmp_agg_results[e_type]:\n",
    "\n",
    "                    self.evaluation_agg_entities_type[e_type][metric] += tmp_agg_results[e_type][metric]\n",
    "\n",
    "                # Calculate precision recall at the individual entity level\n",
    "\n",
    "                # self.evaluation_agg_entities_type[e_type] = compute_precision_recall_wrapper(self.evaluation_agg_entities_type[e_type])\n",
    "\n",
    "        return self.results, self.evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def collect_named_entities(tokens):\n",
    "    \"\"\"\n",
    "    Creates a list of Entity named-tuples, storing the entity type and the start and end\n",
    "    offsets of the entity.\n",
    "    :param tokens: a list of tags\n",
    "    :return: a list of Entity named-tuples\n",
    "    \"\"\"\n",
    "\n",
    "    named_entities = []\n",
    "    start_offset = None\n",
    "    end_offset = None\n",
    "    ent_type = None\n",
    "\n",
    "    for offset, token_tag in enumerate(tokens):\n",
    "\n",
    "        if token_tag == 'O':\n",
    "            if ent_type is not None and start_offset is not None:\n",
    "                end_offset = offset - 1\n",
    "                named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "                start_offset = None\n",
    "                end_offset = None\n",
    "                ent_type = None\n",
    "\n",
    "        elif ent_type is None:\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "\n",
    "        elif ent_type != token_tag[2:] or (ent_type == token_tag[2:] and token_tag[:1] == 'B'):\n",
    "\n",
    "            end_offset = offset - 1\n",
    "            named_entities.append(Entity(ent_type, start_offset, end_offset))\n",
    "\n",
    "            # start of a new entity\n",
    "            ent_type = token_tag[2:]\n",
    "            start_offset = offset\n",
    "            end_offset = None\n",
    "\n",
    "    # catches an entity that goes up until the last token\n",
    "\n",
    "    if ent_type is not None and start_offset is not None and end_offset is None:\n",
    "        named_entities.append(Entity(ent_type, start_offset, len(tokens)-1))\n",
    "\n",
    "    return named_entities\n",
    "\n",
    "\n",
    "def compute_metrics(true_named_entities, pred_named_entities, tags):\n",
    "\n",
    "\n",
    "    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    \n",
    "    evaluation = deepcopy(eval_metrics)\n",
    "\n",
    "    # results by entity type\n",
    "\n",
    "    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n",
    "\n",
    "    # keep track of entities that overlapped\n",
    "\n",
    "    true_which_overlapped_with_pred = []\n",
    "\n",
    "    # Subset into only the tags that we are interested in.\n",
    "    # NOTE: we remove the tags we don't want from both the predicted and the\n",
    "    # true entities. This covers the two cases where mismatches can occur:\n",
    "    #\n",
    "    # 1) Where the model predicts a tag that is not present in the true data\n",
    "    # 2) Where there is a tag in the true data that the model is not capable of\n",
    "    # predicting.\n",
    "\n",
    "    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n",
    "    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # Check each of the potential scenarios in turn. See\n",
    "        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "        # for scenario explanation.\n",
    "\n",
    "        # Scenario I: Exact match between true and pred\n",
    "\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['correct'] += 1\n",
    "\n",
    "            # for the agg. by e_type results\n",
    "            evaluation_agg_entities_type[pred.e_type]['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                pred_range = range(pred.start_offset, pred.end_offset)\n",
    "                true_range = range(true.start_offset, true.end_offset)\n",
    "\n",
    "                # Scenario IV: Offsets match, but entity type is wrong\n",
    "\n",
    "                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n",
    "                        and true.e_type != pred.e_type:\n",
    "\n",
    "                    # overall results\n",
    "                    evaluation['incorrect'] += 1\n",
    "\n",
    "                    # aggregated by entity type results\n",
    "                    evaluation_agg_entities_type[true.e_type]['incorrect'] += 1\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "                    found_overlap = True\n",
    "\n",
    "                    break\n",
    "\n",
    "                # check for an overlap i.e. not exact boundary match, with true entities\n",
    "\n",
    "                elif find_overlap(true_range, pred_range):\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    # Scenario V: There is an overlap (but offsets do not match\n",
    "                    # exactly), and the entity type is the same.\n",
    "                    # 2.1 overlaps with the same entity type\n",
    "\n",
    "                    if pred.e_type == true.e_type:\n",
    "\n",
    "                        # overall results\n",
    "                        evaluation['partial'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    # Scenario VI: Entities overlap, but the entity type is\n",
    "                    # different.\n",
    "\n",
    "                    else:\n",
    "                        # overall results\n",
    "                        evaluation['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        # Results against the true entity\n",
    "\n",
    "                        evaluation_agg_entities_type[true.e_type]['incorrect'] += 1\n",
    "\n",
    "                        # Results against the predicted entity\n",
    "\n",
    "                        # evaluation_agg_entities_type[pred.e_type]['spurious'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "            # Scenario II: Entities are spurious (i.e., over-generated).\n",
    "\n",
    "            if not found_overlap:\n",
    "\n",
    "                # Overall results\n",
    "\n",
    "                evaluation['spurious'] += 1\n",
    "                # Aggregated by entity type results\n",
    "\n",
    "                # NOTE: when pred.e_type is not found in tags\n",
    "                # or when it simply does not appear in the test set, then it is\n",
    "                # spurious, but it is not clear where to assign it at the tag\n",
    "                # level. In this case, it is applied to all target_tags\n",
    "                # found in this example. This will mean that the sum of the\n",
    "                # evaluation_agg_entities will not equal evaluation.\n",
    "                 \n",
    "\n",
    "                evaluation_agg_entities_type[true.e_type]['spurious'] += 1\n",
    "\n",
    "    # Scenario III: Entity was missed entirely.\n",
    "\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['missed'] += 1\n",
    "\n",
    "            # for the agg. by e_type\n",
    "            evaluation_agg_entities_type[true.e_type]['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n",
    "    # overall results, and use these to calculate precision and recall.\n",
    "\n",
    "    evaluation = compute_actual_possible(evaluation)\n",
    "\n",
    "    # Compute 'possible', 'actual', and precision and recall on entity level\n",
    "    # results. Start by cycling through the accumulated results.\n",
    "\n",
    "    for entity_type, entity_level in evaluation_agg_entities_type.items():\n",
    "\n",
    "        # Cycle through the evaluation types for each dict containing entity\n",
    "        # level results.\n",
    "\n",
    "            evaluation_agg_entities_type[entity_type] = compute_actual_possible(\n",
    "                entity_level\n",
    "            )\n",
    "\n",
    "    return evaluation, evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def find_overlap(true_range, pred_range):\n",
    "    \"\"\"Find the overlap between two ranges\n",
    "    Find the overlap between two ranges. Return the overlapping values if\n",
    "    present, else return an empty set().\n",
    "    Examples:\n",
    "    >>> find_overlap((1, 2), (2, 3))\n",
    "    2\n",
    "    >>> find_overlap((1, 2), (3, 4))\n",
    "    set()\n",
    "    \"\"\"\n",
    "\n",
    "    true_set = set(true_range)\n",
    "    pred_set = set(pred_range)\n",
    "\n",
    "    overlaps = true_set.intersection(pred_set)\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "\n",
    "def compute_actual_possible(results):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with actual, possible populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = results['correct']\n",
    "    incorrect = results['incorrect']\n",
    "    partial = results['partial']\n",
    "    missed = results['missed']\n",
    "    spurious = results['spurious']\n",
    "\n",
    "    # Possible: number annotations in the gold-standard which contribute to the\n",
    "    # final score\n",
    "\n",
    "    possible = correct + incorrect + partial + missed\n",
    "\n",
    "    # Actual: number of annotations produced by the NER system\n",
    "\n",
    "    actual = correct + incorrect + partial + spurious\n",
    "\n",
    "    results[\"actual\"] = actual\n",
    "    results[\"possible\"] = possible\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_recall(results, partial_or_type=False):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with precison and recall populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    actual = results[\"actual\"]\n",
    "    possible = results[\"possible\"]\n",
    "    partial = results['partial']\n",
    "    correct = results['correct']\n",
    "\n",
    "    if partial_or_type:\n",
    "        precision = (correct + 0.5 * partial) / actual if actual > 0 else 0\n",
    "        recall = (correct + 0.5 * partial) / possible if possible > 0 else 0\n",
    "\n",
    "    else:\n",
    "        precision = correct / actual if actual > 0 else 0\n",
    "        recall = correct / possible if possible > 0 else 0\n",
    "\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"recall\"] = recall\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_recall_wrapper(results):\n",
    "    \"\"\"\n",
    "    Wraps the compute_precision_recall function and runs on a dict of results\n",
    "    \"\"\"\n",
    "\n",
    "    results_a = {key: compute_precision_recall(value, True) for key, value in results.items() if\n",
    "                 key in ['partial', 'ent_type']}\n",
    "    results_b = {key: compute_precision_recall(value) for key, value in results.items() if\n",
    "                 key in ['strict', 'exact']}\n",
    "\n",
    "    results = {**results_a, **results_b}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-17 19:44:28 root INFO: Imported 1 predictions for 1 true examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'correct': 48,\n",
       "  'incorrect': 0,\n",
       "  'partial': 3,\n",
       "  'missed': 5,\n",
       "  'spurious': 3,\n",
       "  'possible': 56,\n",
       "  'actual': 54,\n",
       "  'precision': 0,\n",
       "  'recall': 0},\n",
       " {'MONEY': {'correct': 48,\n",
       "   'incorrect': 0,\n",
       "   'partial': 3,\n",
       "   'missed': 5,\n",
       "   'spurious': 3,\n",
       "   'possible': 56,\n",
       "   'actual': 54,\n",
       "   'precision': 0,\n",
       "   'recall': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval = Evaluator(true=[list(df.true)], pred =[list(df.pred)], tags=['MONEY'])\n",
    "eval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def compute_metrics(true_named_entities, pred_named_entities, tags):\n",
    "\n",
    "\n",
    "    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    \n",
    "    evaluation = {\n",
    "        'correct': 0,\n",
    "        'incorrect':0,\n",
    "        'partial': 0,\n",
    "        'missed': 0,\n",
    "        'spurious': 0 \n",
    "    }\n",
    "\n",
    "    # results by entity type\n",
    "\n",
    "    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n",
    "\n",
    "    # keep track of entities that overlapped\n",
    "\n",
    "    true_which_overlapped_with_pred = []\n",
    "\n",
    "    # Subset into only the tags that we are interested in.\n",
    "    # NOTE: we remove the tags we don't want from both the predicted and the\n",
    "    # true entities. This covers the two cases where mismatches can occur:\n",
    "    #\n",
    "    # 1) Where the model predicts a tag that is not present in the true data\n",
    "    # 2) Where there is a tag in the true data that the model is not capable of\n",
    "    # predicting.\n",
    "\n",
    "    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n",
    "    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # Check each of the potential scenarios in turn. See\n",
    "        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "        # for scenario explanation.\n",
    "\n",
    "        # Scenario I: Exact match between true and pred\n",
    "\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['correct'] += 1\n",
    "\n",
    "            # for the agg. by e_type results\n",
    "            evaluation_agg_entities_type[pred.e_type]['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                pred_range = range(pred.start_offset, pred.end_offset)\n",
    "                true_range = range(true.start_offset, true.end_offset)\n",
    "\n",
    "                # Scenario IV: Offsets match, but entity type is wrong\n",
    "\n",
    "                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n",
    "                        and true.e_type != pred.e_type:\n",
    "\n",
    "                    # overall results\n",
    "                    evaluation['incorrect'] += 1\n",
    "\n",
    "                    # aggregated by entity type results\n",
    "                    evaluation_agg_entities_type[true.e_type]['incorrect'] += 1\n",
    "\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "                    found_overlap = True\n",
    "\n",
    "                    break\n",
    "\n",
    "                # check for an overlap i.e. not exact boundary match, with true entities\n",
    "\n",
    "                if find_overlap(true_range, pred_range):\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    # Scenario V: There is an overlap (but offsets do not match\n",
    "                    # exactly), and the entity type is the same.\n",
    "                    # 2.1 overlaps with the same entity type\n",
    "\n",
    "                    if pred.e_type == true.e_type:\n",
    "\n",
    "                        # overall results\n",
    "                        evaluation['partial'] += 1\n",
    "\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    # Scenario VI: Entities overlap, but the entity type is\n",
    "                    # different.\n",
    "\n",
    "                    else:\n",
    "                        # overall results\n",
    "                        evaluation['incorrect']+= 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        # Results against the true entity\n",
    "\n",
    "                        evaluation_agg_entities_type[true.e_type]['incorrect'] += 1\n",
    "\n",
    "                        # Results against the predicted entity\n",
    "\n",
    "                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "            # Scenario II: Entities are spurious (i.e., over-generated).\n",
    "\n",
    "            if not found_overlap:\n",
    "\n",
    "                # Overall results\n",
    "\n",
    "                evaluation['spurious'] += 1\n",
    "\n",
    "                # Aggregated by entity type results\n",
    "\n",
    "                # NOTE: when pred.e_type is not found in tags\n",
    "                # or when it simply does not appear in the test set, then it is\n",
    "                # spurious, but it is not clear where to assign it at the tag\n",
    "                # level. In this case, it is applied to all target_tags\n",
    "                # found in this example. This will mean that the sum of the\n",
    "                # evaluation_agg_entities will not equal evaluation.\n",
    "                   \n",
    "                evaluation_agg_entities_type[pred.e_type]['spurious'] += 1\n",
    "\n",
    "    # Scenario III: Entity was missed entirely.\n",
    "\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['missed'] += 1\n",
    "\n",
    "            # for the agg. by e_type\n",
    "            evaluation_agg_entities_type[true.e_type]['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n",
    "    # overall results, and use these to calculate precision and recall.\n",
    "\n",
    "    for eval_type in evaluation:\n",
    "        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n",
    "\n",
    "    # Compute 'possible', 'actual', and precision and recall on entity level\n",
    "    # results. Start by cycling through the accumulated results.\n",
    "\n",
    "    for entity_type, entity_level in evaluation_agg_entities_type.items():\n",
    "\n",
    "        # Cycle through the evaluation types for each dict containing entity\n",
    "        # level results.\n",
    "\n",
    "        for eval_type in entity_level:\n",
    "\n",
    "            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n",
    "                entity_level[eval_type]\n",
    "            )\n",
    "\n",
    "    return evaluation, evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def find_overlap(true_range, pred_range):\n",
    "    \"\"\"Find the overlap between two ranges\n",
    "    Find the overlap between two ranges. Return the overlapping values if\n",
    "    present, else return an empty set().\n",
    "    Examples:\n",
    "    >>> find_overlap((1, 2), (2, 3))\n",
    "    2\n",
    "    >>> find_overlap((1, 2), (3, 4))\n",
    "    set()\n",
    "    \"\"\"\n",
    "\n",
    "    true_set = set(true_range)\n",
    "    pred_set = set(pred_range)\n",
    "\n",
    "    overlaps = true_set.intersection(pred_set)\n",
    "\n",
    "    return overlaps\n",
    "\n",
    "def compute_actual_possible(results):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with actual, possible populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = results['correct']\n",
    "    incorrect = results['incorrect']\n",
    "    partial = results['partial']\n",
    "    missed = results['missed']\n",
    "    spurious = results['spurious']\n",
    "\n",
    "    # Possible: number annotations in the gold-standard which contribute to the\n",
    "    # final score\n",
    "\n",
    "    possible = correct + incorrect + partial + missed\n",
    "\n",
    "    # Actual: number of annotations produced by the NER system\n",
    "\n",
    "    actual = correct + incorrect + partial + spurious\n",
    "\n",
    "    results[\"actual\"] = actual\n",
    "    results[\"possible\"] = possible\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def compute_precision_recall(results, partial_or_type=False):\n",
    "    \"\"\"\n",
    "    Takes a result dict that has been output by compute metrics.\n",
    "    Returns the results dict with precison and recall populated.\n",
    "    When the results dicts is from partial or ent_type metrics, then\n",
    "    partial_or_type=True to ensure the right calculation is used for\n",
    "    calculating precision and recall.\n",
    "    \"\"\"\n",
    "\n",
    "    actual = results[\"actual\"]\n",
    "    possible = results[\"possible\"]\n",
    "    partial = results['partial']\n",
    "    correct = results['correct']\n",
    "\n",
    "    if partial_or_type:\n",
    "        precision = (correct + 0.5 * partial) / actual if actual > 0 else 0\n",
    "        recall = (correct + 0.5 * partial) / possible if possible > 0 else 0\n",
    "\n",
    "    else:\n",
    "        precision = correct / actual if actual > 0 else 0\n",
    "        recall = correct / possible if possible > 0 else 0\n",
    "\n",
    "    results[\"precision\"] = precision\n",
    "    results[\"recall\"] = recall\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = collect_named_entities(df.true)\n",
    "pred = collect_named_entities(df.pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Documents\\UA\\Stage\\Middelner_Github\\Error_analysis\\Errors_bilstm.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m compute_metrics(true, pred, [\u001b[39m'\u001b[39;49m\u001b[39mDATE\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mMONEY\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mLOC\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mPERS\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Documents\\UA\\Stage\\Middelner_Github\\Error_analysis\\Errors_bilstm.ipynb Cell 14\u001b[0m in \u001b[0;36mcompute_metrics\u001b[1;34m(true_named_entities, pred_named_entities, tags)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m# Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \u001b[39m# overall results, and use these to calculate precision and recall.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=157'>158</a>\u001b[0m \u001b[39mfor\u001b[39;00m eval_type \u001b[39min\u001b[39;00m evaluation:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=158'>159</a>\u001b[0m     evaluation[eval_type] \u001b[39m=\u001b[39m compute_actual_possible(evaluation[eval_type])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=160'>161</a>\u001b[0m \u001b[39m# Compute 'possible', 'actual', and precision and recall on entity level\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=161'>162</a>\u001b[0m \u001b[39m# results. Start by cycling through the accumulated results.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=163'>164</a>\u001b[0m \u001b[39mfor\u001b[39;00m entity_type, entity_level \u001b[39min\u001b[39;00m evaluation_agg_entities_type\u001b[39m.\u001b[39mitems():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=164'>165</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=165'>166</a>\u001b[0m     \u001b[39m# Cycle through the evaluation types for each dict containing entity\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m     \u001b[39m# level results.\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Gebruiker\\Documents\\UA\\Stage\\Middelner_Github\\Error_analysis\\Errors_bilstm.ipynb Cell 14\u001b[0m in \u001b[0;36mcompute_actual_possible\u001b[1;34m(results)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=195'>196</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_actual_possible\u001b[39m(results):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=196'>197</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=197'>198</a>\u001b[0m \u001b[39m    Takes a result dict that has been output by compute metrics.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=198'>199</a>\u001b[0m \u001b[39m    Returns the results dict with actual, possible populated.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=201'>202</a>\u001b[0m \u001b[39m    calculating precision and recall.\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=202'>203</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=204'>205</a>\u001b[0m     correct \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39;49m\u001b[39mcorrect\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=205'>206</a>\u001b[0m     incorrect \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39m\u001b[39mincorrect\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/Error_analysis/Errors_bilstm.ipynb#X20sZmlsZQ%3D%3D?line=206'>207</a>\u001b[0m     partial \u001b[39m=\u001b[39m results[\u001b[39m'\u001b[39m\u001b[39mpartial\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "compute_metrics(true, pred, ['DATE', 'MONEY', 'LOC','PERS'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db3ad00c254152fbd946dfe3ddf750e2d6dd1f511dd85499e770b0c6b301697a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
