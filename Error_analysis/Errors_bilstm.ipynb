{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Error_analysis/Bi-LSTM_predictions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      B-DATE       0.93      0.68      0.78        40\n",
      "       B-LOC       0.83      0.56      0.67        85\n",
      "     B-MONEY       0.74      0.64      0.69        53\n",
      "      B-PERS       0.94      0.80      0.86       254\n",
      "      I-DATE       0.96      0.66      0.78       151\n",
      "       I-LOC       1.00      0.10      0.18        10\n",
      "     I-MONEY       0.85      0.93      0.89       123\n",
      "      I-PERS       0.88      0.63      0.73       224\n",
      "           O       0.95      0.99      0.97      4310\n",
      "\n",
      "    accuracy                           0.94      5250\n",
      "   macro avg       0.90      0.67      0.73      5250\n",
      "weighted avg       0.94      0.94      0.93      5250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(list(df.ent), list(df.prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6296296296296297\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DATE       0.53      0.50      0.51        40\n",
      "         LOC       0.76      0.53      0.62        85\n",
      "       MONEY       0.56      0.62      0.59        53\n",
      "        PERS       0.71      0.62      0.66       254\n",
      "\n",
      "   micro avg       0.67      0.59      0.63       432\n",
      "   macro avg       0.64      0.57      0.60       432\n",
      "weighted avg       0.68      0.59      0.63       432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import accuracy_score\n",
    "from seqeval.metrics import classification_report\n",
    "from seqeval.metrics import f1_score\n",
    "\n",
    "print(f1_score([list(df.ent)], [list(df.prediction)]))\n",
    "\n",
    "print(classification_report([list(df.ent)], [list(df.prediction)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = ''\n",
    "\n",
    "for x, y in zip(df.word, df.ent):\n",
    "    true = true + (x.strip() + '\\t' + y.strip() + '\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ''\n",
    "\n",
    "for x, y in zip(df.word, df.prediction):\n",
    "    pred = pred + (x.strip() + '\\t' + y.strip() + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 5250 entries, 0 to 5249\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   word        5250 non-null   object\n",
      " 1   ent         5250 non-null   object\n",
      " 2   prediction  5250 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 164.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-31 11:58:03 root DEBUG: Imported 43970 predictions for 44888 true examples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ent_type': {'correct': 333, 'incorrect': 15, 'partial': 0, 'missed': 84, 'spurious': 30, 'possible': 432, 'actual': 378, 'precision': 0.8809523809523809, 'recall': 0.7708333333333334, 'f1': 0.8222222222222222}, 'partial': {'correct': 263, 'incorrect': 0, 'partial': 85, 'missed': 84, 'spurious': 30, 'possible': 432, 'actual': 378, 'precision': 0.8082010582010583, 'recall': 0.7071759259259259, 'f1': 0.754320987654321}, 'strict': {'correct': 255, 'incorrect': 93, 'partial': 0, 'missed': 84, 'spurious': 30, 'possible': 432, 'actual': 378, 'precision': 0.6746031746031746, 'recall': 0.5902777777777778, 'f1': 0.6296296296296297}, 'exact': {'correct': 263, 'incorrect': 85, 'partial': 0, 'missed': 84, 'spurious': 30, 'possible': 432, 'actual': 378, 'precision': 0.6957671957671958, 'recall': 0.6087962962962963, 'f1': 0.6493827160493828}}\n"
     ]
    }
   ],
   "source": [
    "from nervaluate import Evaluator\n",
    "\n",
    "evaluator = Evaluator(true, pred, tags=['LOC', 'PERS', 'DATE', 'MONEY'],  loader=\"conll\")\n",
    "\n",
    "# Returns overall metrics and metrics for each tag\n",
    "\n",
    "results, results_per_tag = evaluator.evaluate()\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'actual': 219,\n",
      " 'correct': 158,\n",
      " 'f1': 0.7758985200845666,\n",
      " 'incorrect': 0,\n",
      " 'missed': 45,\n",
      " 'partial': 51,\n",
      " 'possible': 254,\n",
      " 'precision': 0.8378995433789954,\n",
      " 'recall': 0.7224409448818898,\n",
      " 'spurious': 10}\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "\n",
    "pp.pprint(results_per_tag['PERS']['partial'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Gebruiker/Documents/UA/Stage/NER-Evaluation/')\n",
    "\n",
    "from ner_evaluation.ner_eval import collect_named_entities\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper, compute_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deepcopy\n",
    "\n",
    "def compute_metrics(true_named_entities, pred_named_entities, tags):\n",
    "\n",
    "\n",
    "    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    \n",
    "    evaluation = {\n",
    "        'correct': 0,\n",
    "        'partial': 0,\n",
    "        'missed': 0,\n",
    "        'spurious': 0 \n",
    "    }\n",
    "\n",
    "    # results by entity type\n",
    "\n",
    "    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n",
    "\n",
    "    # keep track of entities that overlapped\n",
    "\n",
    "    true_which_overlapped_with_pred = []\n",
    "\n",
    "    # Subset into only the tags that we are interested in.\n",
    "    # NOTE: we remove the tags we don't want from both the predicted and the\n",
    "    # true entities. This covers the two cases where mismatches can occur:\n",
    "    #\n",
    "    # 1) Where the model predicts a tag that is not present in the true data\n",
    "    # 2) Where there is a tag in the true data that the model is not capable of\n",
    "    # predicting.\n",
    "\n",
    "    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n",
    "    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # Check each of the potential scenarios in turn. See\n",
    "        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "        # for scenario explanation.\n",
    "\n",
    "        # Scenario I: Exact match between true and pred\n",
    "\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['correct'] += 1\n",
    "\n",
    "            # for the agg. by e_type results\n",
    "            evaluation_agg_entities_type[pred.e_type]['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                pred_range = range(pred.start_offset, pred.end_offset)\n",
    "                true_range = range(true.start_offset, true.end_offset)\n",
    "\n",
    "                # Scenario IV: Offsets match, but entity type is wrong\n",
    "\n",
    "                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n",
    "                        and true.e_type != pred.e_type:\n",
    "\n",
    "                    # overall results\n",
    "                    evaluation['strict']['incorrect'] += 1\n",
    "                    evaluation['ent_type']['incorrect'] += 1\n",
    "                    evaluation['partial']['correct'] += 1\n",
    "                    evaluation['exact']['correct'] += 1\n",
    "\n",
    "                    # aggregated by entity type results\n",
    "                    evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['partial']['correct'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['exact']['correct'] += 1\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "                    found_overlap = True\n",
    "\n",
    "                    break\n",
    "\n",
    "                # check for an overlap i.e. not exact boundary match, with true entities\n",
    "\n",
    "                if find_overlap(true_range, pred_range):\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    # Scenario V: There is an overlap (but offsets do not match\n",
    "                    # exactly), and the entity type is the same.\n",
    "                    # 2.1 overlaps with the same entity type\n",
    "\n",
    "                    if pred.e_type == true.e_type:\n",
    "\n",
    "                        # overall results\n",
    "                        evaluation['partial'] += 1\n",
    "\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    # Scenario VI: Entities overlap, but the entity type is\n",
    "                    # different.\n",
    "\n",
    "                    else:\n",
    "                        # overall results\n",
    "                        evaluation['strict']['incorrect'] += 1\n",
    "                        evaluation['ent_type']['incorrect'] += 1\n",
    "                        evaluation['partial']['partial'] += 1\n",
    "                        evaluation['exact']['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        # Results against the true entity\n",
    "\n",
    "                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n",
    "\n",
    "                        # Results against the predicted entity\n",
    "\n",
    "                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "            # Scenario II: Entities are spurious (i.e., over-generated).\n",
    "\n",
    "            if not found_overlap:\n",
    "\n",
    "                # Overall results\n",
    "\n",
    "                evaluation['strict']['spurious'] += 1\n",
    "                evaluation['ent_type']['spurious'] += 1\n",
    "                evaluation['partial']['spurious'] += 1\n",
    "                evaluation['exact']['spurious'] += 1\n",
    "\n",
    "                # Aggregated by entity type results\n",
    "\n",
    "                # NOTE: when pred.e_type is not found in tags\n",
    "                # or when it simply does not appear in the test set, then it is\n",
    "                # spurious, but it is not clear where to assign it at the tag\n",
    "                # level. In this case, it is applied to all target_tags\n",
    "                # found in this example. This will mean that the sum of the\n",
    "                # evaluation_agg_entities will not equal evaluation.\n",
    "\n",
    "                for true in tags:                    \n",
    "\n",
    "                    evaluation_agg_entities_type[true]['strict']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['ent_type']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['partial']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['exact']['spurious'] += 1\n",
    "\n",
    "    # Scenario III: Entity was missed entirely.\n",
    "\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['strict']['missed'] += 1\n",
    "            evaluation['ent_type']['missed'] += 1\n",
    "            evaluation['partial']['missed'] += 1\n",
    "            evaluation['exact']['missed'] += 1\n",
    "\n",
    "            # for the agg. by e_type\n",
    "            evaluation_agg_entities_type[true.e_type]['strict']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['ent_type']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['partial']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['exact']['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n",
    "    # overall results, and use these to calculate precision and recall.\n",
    "\n",
    "    for eval_type in evaluation:\n",
    "        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n",
    "\n",
    "    # Compute 'possible', 'actual', and precision and recall on entity level\n",
    "    # results. Start by cycling through the accumulated results.\n",
    "\n",
    "    for entity_type, entity_level in evaluation_agg_entities_type.items():\n",
    "\n",
    "        # Cycle through the evaluation types for each dict containing entity\n",
    "        # level results.\n",
    "\n",
    "        for eval_type in entity_level:\n",
    "\n",
    "            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n",
    "                entity_level[eval_type]\n",
    "            )\n",
    "\n",
    "    return evaluation, evaluation_agg_entities_type\n",
    "\n",
    "\n",
    "def find_overlap(true_range, pred_range):\n",
    "    \"\"\"Find the overlap between two ranges\n",
    "    Find the overlap between two ranges. Return the overlapping values if\n",
    "    present, else return an empty set().\n",
    "    Examples:\n",
    "    >>> find_overlap((1, 2), (2, 3))\n",
    "    2\n",
    "    >>> find_overlap((1, 2), (3, 4))\n",
    "    set()\n",
    "    \"\"\"\n",
    "\n",
    "    true_set = set(true_range)\n",
    "    pred_set = set(pred_range)\n",
    "\n",
    "    overlaps = true_set.intersection(pred_set)\n",
    "\n",
    "    return overlaps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = collect_named_entities(df.ent)\n",
    "pred = collect_named_entities(df.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC {'strict': {'correct': 45, 'incorrect': 5, 'partial': 0, 'missed': 35, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 106, 'possible': 85}, 'ent_type': {'correct': 45, 'incorrect': 5, 'partial': 0, 'missed': 35, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 106, 'possible': 85}, 'partial': {'correct': 50, 'incorrect': 0, 'partial': 0, 'missed': 35, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 106, 'possible': 85}, 'exact': {'correct': 50, 'incorrect': 0, 'partial': 0, 'missed': 35, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 106, 'possible': 85}}\n",
      "PERS {'strict': {'correct': 157, 'incorrect': 35, 'partial': 0, 'missed': 62, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 248, 'possible': 254}, 'ent_type': {'correct': 191, 'incorrect': 1, 'partial': 0, 'missed': 62, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 248, 'possible': 254}, 'partial': {'correct': 158, 'incorrect': 0, 'partial': 34, 'missed': 62, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 248, 'possible': 254}, 'exact': {'correct': 158, 'incorrect': 34, 'partial': 0, 'missed': 62, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 248, 'possible': 254}}\n",
      "DATE {'strict': {'correct': 20, 'incorrect': 13, 'partial': 0, 'missed': 12, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 89, 'possible': 45}, 'ent_type': {'correct': 31, 'incorrect': 2, 'partial': 0, 'missed': 12, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 89, 'possible': 45}, 'partial': {'correct': 22, 'incorrect': 0, 'partial': 11, 'missed': 12, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 89, 'possible': 45}, 'exact': {'correct': 22, 'incorrect': 11, 'partial': 0, 'missed': 12, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 89, 'possible': 45}}\n",
      "MONEY {'strict': {'correct': 33, 'incorrect': 14, 'partial': 0, 'missed': 6, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 103, 'possible': 53}, 'ent_type': {'correct': 47, 'incorrect': 0, 'partial': 0, 'missed': 6, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 103, 'possible': 53}, 'partial': {'correct': 33, 'incorrect': 0, 'partial': 14, 'missed': 6, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 103, 'possible': 53}, 'exact': {'correct': 33, 'incorrect': 14, 'partial': 0, 'missed': 6, 'spurious': 56, 'precision': 0, 'recall': 0, 'actual': 103, 'possible': 53}}\n"
     ]
    }
   ],
   "source": [
    "for key, value in ent_agg.items():\n",
    "    print(key, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db3ad00c254152fbd946dfe3ddf750e2d6dd1f511dd85499e770b0c6b301697a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
