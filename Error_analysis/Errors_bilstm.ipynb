{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Gebruiker\\\\Documents\\\\UA\\\\Stage\\\\NER-Evaluation'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Gebruiker/Documents/UA/Stage/Middelner_Github/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Error_analysis/Bi-LSTM_predictions.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:/Users/Gebruiker/Documents/UA/Stage/NER-Evaluation/')\n",
    "\n",
    "from ner_evaluation.ner_eval import collect_named_entities\n",
    "from ner_evaluation.ner_eval import compute_metrics\n",
    "from ner_evaluation.ner_eval import compute_precision_recall_wrapper, compute_precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(true_named_entities, pred_named_entities, tags):\n",
    "\n",
    "\n",
    "    eval_metrics = {'correct': 0, 'incorrect': 0, 'partial': 0, 'missed': 0, 'spurious': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "    # overall results\n",
    "    \n",
    "    evaluation = {\n",
    "        'strict': deepcopy(eval_metrics),\n",
    "        'ent_type': deepcopy(eval_metrics),\n",
    "        'partial': deepcopy(eval_metrics),\n",
    "        'exact': deepcopy(eval_metrics)\n",
    "    }\n",
    "\n",
    "    # results by entity type\n",
    "\n",
    "    evaluation_agg_entities_type = {e: deepcopy(evaluation) for e in tags}\n",
    "\n",
    "    # keep track of entities that overlapped\n",
    "\n",
    "    true_which_overlapped_with_pred = []\n",
    "\n",
    "    # Subset into only the tags that we are interested in.\n",
    "    # NOTE: we remove the tags we don't want from both the predicted and the\n",
    "    # true entities. This covers the two cases where mismatches can occur:\n",
    "    #\n",
    "    # 1) Where the model predicts a tag that is not present in the true data\n",
    "    # 2) Where there is a tag in the true data that the model is not capable of\n",
    "    # predicting.\n",
    "\n",
    "    true_named_entities = [ent for ent in true_named_entities if ent.e_type in tags]\n",
    "    pred_named_entities = [ent for ent in pred_named_entities if ent.e_type in tags]\n",
    "\n",
    "    # go through each predicted named-entity\n",
    "\n",
    "    for pred in pred_named_entities:\n",
    "        found_overlap = False\n",
    "\n",
    "        # Check each of the potential scenarios in turn. See\n",
    "        # http://www.davidsbatista.net/blog/2018/05/09/Named_Entity_Evaluation/\n",
    "        # for scenario explanation.\n",
    "\n",
    "        # Scenario I: Exact match between true and pred\n",
    "\n",
    "        if pred in true_named_entities:\n",
    "            true_which_overlapped_with_pred.append(pred)\n",
    "            evaluation['strict']['correct'] += 1\n",
    "            evaluation['ent_type']['correct'] += 1\n",
    "            evaluation['exact']['correct'] += 1\n",
    "            evaluation['partial']['correct'] += 1\n",
    "\n",
    "            # for the agg. by e_type results\n",
    "            evaluation_agg_entities_type[pred.e_type]['strict']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['ent_type']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['exact']['correct'] += 1\n",
    "            evaluation_agg_entities_type[pred.e_type]['partial']['correct'] += 1\n",
    "\n",
    "        else:\n",
    "\n",
    "            # check for overlaps with any of the true entities\n",
    "\n",
    "            for true in true_named_entities:\n",
    "\n",
    "                pred_range = range(pred.start_offset, pred.end_offset)\n",
    "                true_range = range(true.start_offset, true.end_offset)\n",
    "\n",
    "                # Scenario IV: Offsets match, but entity type is wrong\n",
    "\n",
    "                if true.start_offset == pred.start_offset and pred.end_offset == true.end_offset \\\n",
    "                        and true.e_type != pred.e_type:\n",
    "\n",
    "                    # overall results\n",
    "                    evaluation['strict']['incorrect'] += 1\n",
    "                    evaluation['ent_type']['incorrect'] += 1\n",
    "                    evaluation['partial']['correct'] += 1\n",
    "                    evaluation['exact']['correct'] += 1\n",
    "\n",
    "                    # aggregated by entity type results\n",
    "                    evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['partial']['correct'] += 1\n",
    "                    evaluation_agg_entities_type[true.e_type]['exact']['correct'] += 1\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "                    found_overlap = True\n",
    "\n",
    "                    break\n",
    "\n",
    "                # check for an overlap i.e. not exact boundary match, with true entities\n",
    "\n",
    "                elif find_overlap(true_range, pred_range):\n",
    "\n",
    "                    true_which_overlapped_with_pred.append(true)\n",
    "\n",
    "                    # Scenario V: There is an overlap (but offsets do not match\n",
    "                    # exactly), and the entity type is the same.\n",
    "                    # 2.1 overlaps with the same entity type\n",
    "\n",
    "                    if pred.e_type == true.e_type:\n",
    "\n",
    "                        # overall results\n",
    "                        evaluation['strict']['incorrect'] += 1\n",
    "                        evaluation['ent_type']['correct'] += 1\n",
    "                        evaluation['partial']['partial'] += 1\n",
    "                        evaluation['exact']['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['ent_type']['correct'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "                    # Scenario VI: Entities overlap, but the entity type is\n",
    "                    # different.\n",
    "\n",
    "                    else:\n",
    "                        # overall results\n",
    "                        evaluation['strict']['incorrect'] += 1\n",
    "                        evaluation['ent_type']['incorrect'] += 1\n",
    "                        evaluation['partial']['partial'] += 1\n",
    "                        evaluation['exact']['incorrect'] += 1\n",
    "\n",
    "                        # aggregated by entity type results\n",
    "                        # Results against the true entity\n",
    "\n",
    "                        evaluation_agg_entities_type[true.e_type]['strict']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['partial']['partial'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['ent_type']['incorrect'] += 1\n",
    "                        evaluation_agg_entities_type[true.e_type]['exact']['incorrect'] += 1\n",
    "\n",
    "                        # Results against the predicted entity\n",
    "\n",
    "                        # evaluation_agg_entities_type[pred.e_type]['strict']['spurious'] += 1\n",
    "\n",
    "                        found_overlap = True\n",
    "\n",
    "                        break\n",
    "\n",
    "            # Scenario II: Entities are spurious (i.e., over-generated).\n",
    "\n",
    "            if not found_overlap:\n",
    "\n",
    "                # Overall results\n",
    "\n",
    "                evaluation['strict']['spurious'] += 1\n",
    "                evaluation['ent_type']['spurious'] += 1\n",
    "                evaluation['partial']['spurious'] += 1\n",
    "                evaluation['exact']['spurious'] += 1\n",
    "\n",
    "                # Aggregated by entity type results\n",
    "\n",
    "                # NOTE: when pred.e_type is not found in tags\n",
    "                # or when it simply does not appear in the test set, then it is\n",
    "                # spurious, but it is not clear where to assign it at the tag\n",
    "                # level. In this case, it is applied to all target_tags\n",
    "                # found in this example. This will mean that the sum of the\n",
    "                # evaluation_agg_entities will not equal evaluation.\n",
    "\n",
    "                for true in tags:                    \n",
    "\n",
    "                    evaluation_agg_entities_type[true]['strict']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['ent_type']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['partial']['spurious'] += 1\n",
    "                    evaluation_agg_entities_type[true]['exact']['spurious'] += 1\n",
    "\n",
    "    # Scenario III: Entity was missed entirely.\n",
    "\n",
    "    for true in true_named_entities:\n",
    "        if true in true_which_overlapped_with_pred:\n",
    "            continue\n",
    "        else:\n",
    "            # overall results\n",
    "            evaluation['strict']['missed'] += 1\n",
    "            evaluation['ent_type']['missed'] += 1\n",
    "            evaluation['partial']['missed'] += 1\n",
    "            evaluation['exact']['missed'] += 1\n",
    "\n",
    "            # for the agg. by e_type\n",
    "            evaluation_agg_entities_type[true.e_type]['strict']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['ent_type']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['partial']['missed'] += 1\n",
    "            evaluation_agg_entities_type[true.e_type]['exact']['missed'] += 1\n",
    "\n",
    "    # Compute 'possible', 'actual' according to SemEval-2013 Task 9.1 on the\n",
    "    # overall results, and use these to calculate precision and recall.\n",
    "\n",
    "    for eval_type in evaluation:\n",
    "        evaluation[eval_type] = compute_actual_possible(evaluation[eval_type])\n",
    "\n",
    "    # Compute 'possible', 'actual', and precision and recall on entity level\n",
    "    # results. Start by cycling through the accumulated results.\n",
    "\n",
    "    for entity_type, entity_level in evaluation_agg_entities_type.items():\n",
    "\n",
    "        # Cycle through the evaluation types for each dict containing entity\n",
    "        # level results.\n",
    "\n",
    "        for eval_type in entity_level:\n",
    "\n",
    "            evaluation_agg_entities_type[entity_type][eval_type] = compute_actual_possible(\n",
    "                entity_level[eval_type]\n",
    "            )\n",
    "\n",
    "    return evaluation, evaluation_agg_entities_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = collect_named_entities(df.ent)\n",
    "pred = collect_named_entities(df.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Entity(e_type='PERS', start_offset=2, end_offset=2), Entity(e_type='PERS', start_offset=7, end_offset=7), Entity(e_type='LOC', start_offset=12, end_offset=12), Entity(e_type='LOC', start_offset=22, end_offset=22), Entity(e_type='PERS', start_offset=29, end_offset=30), Entity(e_type='MONEY', start_offset=35, end_offset=37), Entity(e_type='DATE', start_offset=46, end_offset=50), Entity(e_type='PERS', start_offset=83, end_offset=83), Entity(e_type='DATE', start_offset=109, end_offset=118), Entity(e_type='PERS', start_offset=121, end_offset=121)]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "db3ad00c254152fbd946dfe3ddf750e2d6dd1f511dd85499e770b0c6b301697a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
